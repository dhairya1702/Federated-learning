
# ğŸš€ Enhancing Federated Learning Security with Trust & Reputation Mechanisms 

This project demonstrates a **federated learning** setup that introduces a Trust and Reputation-based approach that extends FedAvg to dynamically filter malicious or low-quality clients, boosting model reliability and convergence in adversarial environments.  
The **MNIST dataset** ğŸ–¼ï¸ is partitioned across multiple clients, each training a model on their local data before sending updates to the central **server** ğŸ–¥ï¸ for aggregation.  

To simulate **adversarial clients**, **Clients 9 & 10** contain **poisoned data** ğŸ§ª (label flipping).  

The server implements a **Trust and Reputation Mechanism** ğŸ† to mitigate their influence.

---

## âš™ï¸ Setup & Requirements 

1. **Dependencies**:
   - Python 3.7+
   - Install the required libraries:
     ```bash
     pip install flwr torch torchvision matplotlib pandas
     ```

2. **Folder Structure**:
   - Place this script and all code files in a single directory.
   - Ensure a subdirectory called `data/mnist/` exists within this directory to store the MNIST dataset.
   - 

## ğŸ“Š Data Preparation

1. **ğŸ“‰ Download and Partition the MNIST Dataset**:
   - Run `prepare_data.py` to download the MNIST dataset and split it into partitions for each client:
     ```bash
     python prepare_data.py
     ```
This will:

     âœ… Download the MNIST dataset ğŸ–¼ï¸
     âœ… Create 10 partitions inside data/mnist/ ğŸ“‚
     âœ… Poison Clients 9 & 10 with random label flipping ğŸ§ª


## ğŸ” Code Overview  

### ğŸ–¥ï¸ **Server Code** (`server.py`)  
- Initializes and starts the **Federated Learning Server** ğŸŒ¸.  
- Implements a **custom aggregation strategy** with **Trust & Reputation mechanisms** ğŸ†.  
- Generates **accuracy & loss plots** ğŸ“Š.  

### ğŸ¤– **Client Code** (`client.py`)  
- Each client trains a **PyTorch-based model** locally and communicates with the server.  
- **Clients 9 & 10** contain **poisoned data** for adversarial testing ğŸ§ª.  

## ğŸš€ Running the Code  

1. **ğŸ Start the Server**:
   - Open a terminal and run the server:
     ```bash
     python server.py
     ```

2. **ğŸ¤– Start the Clients**:
   - In separate terminals, start each client by setting a unique client ID:
     ```bash
     CLIENT_ID=0 python client.py
     CLIENT_ID=1 python client.py
     ...
     CLIENT_ID=9 python client.py
     ```
   - You need 10 terminals or terminal tabs to start each client with IDs from 0 to 9. 

## ğŸ“ˆ Results & Plots  


| Scenario              | Accuracy (%) | Observation                                  |
|-----------------------|--------------|----------------------------------------------|
| Baseline (FedAvg)     | ~80%         | Poisoned clients degraded model performance  |
| Trust-Enhanced FedAvg | ~87%         | Malicious clients filtered, improved accuracy|

### ğŸ” Additional Insights

- ğŸ“‰ **Global loss dropped** from 1.0 to near 0.02 in just 3 rounds.
- ğŸ§‘â€âš•ï¸ **Benign clients** maintained steady trust; adversaries showed steep trust decay.
- ğŸ›°ï¸ **Low overhead** in communication and computation.


After all **federated rounds** complete, the **server** will generate the following visualizations:  

ğŸ”¹ **Loss Trends**  
   - ğŸ“‰ **Loss per client** over rounds  
   - ğŸ“Š **Average loss** across all clients  

ğŸ”¹ **Accuracy Trends**  
   - âœ… **Evaluation accuracy per client** over rounds  
   - ğŸ“ˆ **Average evaluation accuracy** among all clients  

These plots help **analyze model performance** and the **impact of poisoned clients** on federated learning.  

## ğŸš§ Future Work

- ğŸ” Add cryptographic secure aggregation
- ğŸŒ Scale to 1000+ clients for IoT-scale simulation
- ğŸ¤– Handle adaptive adversaries and sybil attacks
- ğŸ“¡ Optimize further for edge deployments
- ğŸª Incorporate explainability into trust scores

---

